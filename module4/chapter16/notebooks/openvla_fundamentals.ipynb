{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: OpenVLA Fundamentals â€“ Vision-Based Action Generation\n",
    "\n",
    "This notebook introduces the fundamentals of Vision-Language-Action (VLA) models using OpenVLA. You'll learn to run OpenVLA in a notebook environment and experiment with vision-based action prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "# Note: This might take a few minutes\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers\n",
    "!pip install huggingface_hub\n",
    "!pip install accelerate\n",
    "!pip install numpy matplotlib pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "\n",
    "# Import our custom modules\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from utils.vla_interface import VLAInterface, VLAConfig\n",
    "from utils.common_data_structures import VisionInput, LanguageInput, ActionOutput, VLAPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if CUDA is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Vision-Language-Action Models\n",
    "\n",
    "VLA models combine visual perception, language understanding, and action generation in a unified framework. This allows robots to understand natural language commands and execute appropriate physical actions based on visual input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up OpenVLA Environment and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the VLA Interface\n",
    "vla_config = VLAConfig(\n",
    "    model_name=\"openvla/openvla-7b\",  # Using the 7B parameter model\n",
    "    device=device,\n",
    "    precision=\"fp16\"  # Use half precision to save memory\n",
    ")\n",
    "\n",
    "vla_interface = VLAInterface(vla_config)\n",
    "print(\"VLA Interface initialized with config:\", vla_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading and Testing the OpenVLA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "# Load the OpenVLA model (this might take a minute or two)\n",
    "try:\n",
    "    vla_interface.load_model()\n",
    "    print(\"OpenVLA model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"\\nNote: If running on CPU, model loading may take significantly longer.\")\n",
    "    print(\"Consider using a GPU-enabled environment for faster inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding VLA Action Spaces and Representations\n",
    "\n",
    "VLA models output actions in a continuous space that needs to be mapped to specific robot commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example of action space representation\n",
    "# In this example, we'll create a mock action for demonstration\n",
    "mock_action = np.random.rand(7)  # 7-DOF action space\n",
    "print(f\"Example action vector: {mock_action}\")\n",
    "print(f\"Action vector shape: {mock_action.shape}\")\n",
    "\n",
    "# Map to robot joint space using our utility\n",
    "from utils.vla_interface import VLAActionSpaceMapper\n",
    "mapper = VLAActionSpaceMapper(\"athena\")\n",
    "robot_action = mapper.vla_to_robot_action(mock_action)\n",
    "print(f\"Mapped to robot action: {robot_action['joint_positions'][:6]}... (showing first 6 joints)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Basic VLA Inference: From Images to Joint Commands\n",
    "\n",
    "Let's run our first VLA inference with a sample image and instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a sample image (in practice, you'd load a real image)\n",
    "# For this example, we'll create a synthetic image\n",
    "sample_image = Image.new('RGB', (224, 224), color='red')\n",
    "sample_instruction = \"Move the robot arm to the left\"\n",
    "\n",
    "print(f\"Sample instruction: {sample_instruction}\")\n",
    "print(\"Sample image created (red square for demonstration)\")\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(sample_image)\n",
    "plt.title(\"Sample Image for VLA Inference\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Perform VLA inference\n",
    "if vla_interface.is_initialized:\n",
    "    try:\n",
    "        # Convert PIL image to numpy array for processing\n",
    "        img_array = np.array(sample_image)\n",
    "        \n",
    "        # Get action prediction from VLA model\n",
    "        action_prediction = vla_interface.predict_action(img_array, sample_instruction)\n",
    "        \n",
    "        print(f\"Action prediction shape: {action_prediction.shape}\")\n",
    "        print(f\"Action prediction: {action_prediction}\")\n",
    "        \n",
    "        # Create a VLAPrediction object with the results\n",
    "        vision_input = VisionInput(image=img_array)\n",
    "        language_input = LanguageInput(text=sample_instruction)\n",
    "        action_output = ActionOutput(joint_positions=action_prediction.tolist())\n",
    "        \n",
    "        vla_prediction = VLAPrediction(\n",
    "            vision_input=vision_input,\n",
    "            language_input=language_input,\n",
    "            action_output=action_output\n",
    "        )\n",
    "        \n",
    "        print(\"VLA prediction created successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during VLA inference: {e}\")\n",
    "        print(\"\\nThis might happen if the model wasn't loaded properly or resources are limited.\")\n",
    "else:\n",
    "    print(\"VLA interface not initialized. Please run the model loading step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Manipulation Tasks with VLA Models\n",
    "\n",
    "Now let's try a more realistic manipulation scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example manipulation scenario\n",
    "manipulation_scenarios = [\n",
    "    \"Pick up the red cup on the table\",\n",
    "    \"Move the robot arm to grasp the object\",\n",
    "    \"Place the item in the box\"\n",
    "]\n",
    "\n",
    "print(\"Sample manipulation scenarios:\")\n",
    "for i, scenario in enumerate(manipulation_scenarios, 1):\n",
    "    print(f\"{i}. {scenario}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics for VLA Performance\n",
    "\n",
    "In a real implementation, we would evaluate the VLA model's performance on various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a simple evaluation function\n",
    "def evaluate_vla_performance(predictions, targets):\n",
    "    \"\"\"\n",
    "    Simple evaluation function for VLA predictions\n",
    "    In practice, this would be much more complex\n",
    "    \"\"\"\n",
    "    if len(predictions) == 0:\n",
    "        return {\"error\": \"No predictions to evaluate\"}\n",
    "    \n",
    "    # Calculate mean absolute error if targets are provided\n",
    "    if len(predictions) == len(targets):\n",
    "        errors = [np.abs(p - t).mean() for p, t in zip(predictions, targets)]\n",
    "        mae = np.mean(errors)\n",
    "        return {\"mean_absolute_error\": mae}\n",
    "    else:\n",
    "        return {\"prediction_count\": len(predictions)}\n",
    "\n",
    "# Example evaluation\n",
    "sample_predictions = [np.random.rand(7) for _ in range(5)]\n",
    "# In a real scenario, we would have target actions to compare against\n",
    "evaluation_result = evaluate_vla_performance(sample_predictions, [])\n",
    "print(\"Evaluation result:\", evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Troubleshooting Common VLA Issues\n",
    "\n",
    "Here are some common issues and solutions when working with VLA models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 1: Memory (VRAM) Limitations\n",
    "- Solution: Use model quantization or smaller batch sizes\n",
    "- In our configuration manager, we already handle this automatically for different hardware tiers\n",
    "\n",
    "### Issue 2: Slow Inference\n",
    "- Solution: Optimize precision settings (FP16 vs FP32)\n",
    "- Use hardware-specific optimizations\n",
    "\n",
    "### Issue 3: Model Not Producing Expected Results\n",
    "- Ensure image preprocessing matches training conditions\n",
    "- Check if natural language instructions are clear and specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've covered the fundamentals of VLA models using OpenVLA:\n",
    "1. Set up the OpenVLA environment\n",
    "2. Loaded and tested the model\n",
    "3. Understood action space representations\n",
    "4. Performed basic inference\n",
    "5. Explored manipulation tasks\n",
    "6. Considered evaluation metrics\n",
    "7. Reviewed common troubleshooting steps\n",
    "\n",
    "In the next chapter, we'll integrate language understanding to condition VLA models on text prompts for more sophisticated manipulation tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}