---
sidebar_position: 18
---

# Chapter 18: Voice-to-Action Pipeline â€“ Speech Recognition and Natural Language Understanding

## Learning Objectives

After completing this chapter, you will be able to:
- Integrate speech-to-text systems with VLA models for voice command processing
- Implement real-time voice processing pipelines for robot interaction
- Design natural language understanding components for action planning
- Optimize voice processing for noisy environments
- Build conversational interfaces for robot control

## 18.1 Speech-to-Text Integration for Robotics

Speech-to-text integration enables robots to understand and respond to natural spoken commands, creating a more intuitive human-robot interaction.

Let's implement the speech-to-text integration:

```python
# module4/chapter18/code/speech_to_text.py
import whisper
import torch
import pyaudio
import wave
import threading
import queue
from transformers import AutoTokenizer

class VoiceToActionPipeline:
    def __init__(self, model_size="large", device="cuda"):
        # Load Whisper model for speech recognition
        self.speech_model = whisper.load_model(model_size).to(device)
        self.tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")
        
        # Audio parameters
        self.chunk = 1024
        self.format = pyaudio.paInt16
        self.channels = 1
        self.rate = 16000
        
        # Audio queue for real-time processing
        self.audio_queue = queue.Queue()
        self.is_listening = False

    def record_audio(self, duration=5):
        """
        Record audio from microphone for specified duration
        """
        p = pyaudio.PyAudio()
        
        stream = p.open(
            format=self.format,
            channels=self.channels,
            rate=self.rate,
            input=True,
            frames_per_buffer=self.chunk
        )
        
        frames = []
        for _ in range(0, int(self.rate / self.chunk * duration)):
            data = stream.read(self.chunk)
            frames.append(data)
        
        stream.stop_stream()
        stream.close()
        p.terminate()
        
        # Save to temporary file for Whisper processing
        filename = "temp_audio.wav"
        wf = wave.open(filename, 'wb')
        wf.setnchannels(self.channels)
        wf.setsampwidth(p.get_sample_size(self.format))
        wf.setframerate(self.rate)
        wf.writeframes(b''.join(frames))
        wf.close()
        
        return filename
    
    def transcribe_audio(self, audio_file):
        """
        Transcribe audio file to text
        """
        result = self.speech_model.transcribe(audio_file)
        return result["text"]
    
    def process_voice_command(self, audio_file):
        """
        Process voice command and return text for VLA conditioning
        """
        transcription = self.transcribe_audio(audio_file)
        return transcription
```

## 18.2 Natural Language Understanding for Action Planning

Natural language understanding bridges the gap between voice commands and robot actions.

## 18.3 Multi-Modal Processing

Combining speech, vision, and contextual information for robust command execution.

## 18.4 Real-Time Processing Optimization

Optimizing the voice-to-action pipeline for real-time performance.

## 18.5 Safety and Privacy in Voice Interaction

Ensuring safe and private voice-activated robot control.

## Summary

Chapter 18 introduced the implementation of voice-to-action pipelines, integrating speech recognition with VLA models to enable natural language control of robots. We explored real-time processing considerations, conversational interfaces, and safety aspects of voice-controlled robotic systems.